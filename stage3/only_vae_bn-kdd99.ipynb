{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code directly taken from: https://jmetzen.github.io/2015-11-27/vae.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cya/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "\n",
    "### train only on cpu\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-29_20:06 \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "print(timestamp, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wrapper for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load dataset\n",
    "\n",
    "class KDD99Dataset:\n",
    "    def __init__(self, filename_train, filename_test):\n",
    "        with np.load(filename_train) as data: \n",
    "            self.x_train = data[\"x_train\"]\n",
    "            \n",
    "        with np.load(filename_test) as data: \n",
    "            self.x_test = data[\"x_test\"]\n",
    "            \n",
    "            \n",
    "    def train_num_examples(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def train_next_batch(self,batch_size):\n",
    "        choices = np.random.choice(len(self.x_train), size=batch_size, replace=False)\n",
    "\n",
    "        batch_x_train = self.x_train[choices]\n",
    "\n",
    "        batch_train = (batch_x_train, None)\n",
    "        return batch_train\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_next_batch(self,batch_size):\n",
    "        choices = np.random.choice(len(self.x_test), size=batch_size, replace=False)\n",
    "\n",
    "        batch_x_test = self.x_test[choices]\n",
    "        \n",
    "        batch_test = (batch_x_test, None) \n",
    "        return batch_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd99 = KDD99Dataset(\"../datasets/kddcup/kdd99_train-randomState_None.npz\", \n",
    "                     \"../datasets/kddcup/kdd99_test-randomState_None.npz\")\n",
    "\n",
    "n_samples = kdd99.train_num_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we define now a class \"VariationalAutoencoder\" with a [sklearn](http://scikit-learn.org)-like interface that can be trained incrementally with mini-batches using partial_fit. The trained model can be used to reconstruct unseen input, to generate new samples, and to map inputs to the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(network_weights[\"weights_recog\"], \n",
    "                                      network_weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(network_weights[\"weights_gener\"],\n",
    "                                    network_weights[\"biases_gener\"])\n",
    "    def save_weights(self, filename = \"./\" + timestamp + \"-tfsave\"):  \n",
    "        ### save model weights\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, filename)\n",
    "        print(\"saved to:\",filename)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_weights(self, filename = \"./\" + timestamp + \"-tfsave.meta\"):\n",
    "        ### load weights for test notebook\n",
    "        saver = tf.train.import_meta_graph(filename)\n",
    "        saver.restore(self.sess, tf.train.latest_checkpoint(\"./\"))\n",
    "            \n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, n_hidden_recog_3, n_hidden_recog_4,\n",
    "                            n_hidden_gener_1,  n_hidden_gener_2, n_hidden_gener_3, n_hidden_gener_4,\n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'h3': tf.Variable(xavier_init(n_hidden_recog_2, n_hidden_recog_3)),\n",
    "            'h4': tf.Variable(xavier_init(n_hidden_recog_3, n_hidden_recog_4)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_4, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_4, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'b3': tf.Variable(tf.zeros([n_hidden_recog_3], dtype=tf.float32)),\n",
    "            'b4': tf.Variable(tf.zeros([n_hidden_recog_4], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'h3': tf.Variable(xavier_init(n_hidden_gener_2, n_hidden_gener_3)),\n",
    "            'h4': tf.Variable(xavier_init(n_hidden_gener_3, n_hidden_gener_4)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_4, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_4, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'b3': tf.Variable(tf.zeros([n_hidden_gener_3], dtype=tf.float32)),\n",
    "            'b4': tf.Variable(tf.zeros([n_hidden_gener_4], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        \n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_1)\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(batch_normed, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_2)\n",
    "        layer_3 = self.transfer_fct(tf.add(tf.matmul(batch_normed, weights['h3']), \n",
    "                                           biases['b3'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_3)\n",
    "        layer_4 = self.transfer_fct(tf.add(tf.matmul(batch_normed, weights['h4']), \n",
    "                                           biases['b4'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_4)\n",
    "        z_mean = tf.add(tf.matmul(batch_normed, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(batch_normed, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_1)\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(batch_normed, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_2)\n",
    "        layer_3 = self.transfer_fct(tf.add(tf.matmul(batch_normed, weights['h3']), \n",
    "                                           biases['b3'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_3)\n",
    "        layer_4 = self.transfer_fct(tf.add(tf.matmul(batch_normed, weights['h4']), \n",
    "                                           biases['b4'])) \n",
    "        batch_normed = tf.keras.layers.BatchNormalization()(layer_4)\n",
    "        x_reconstr_mean = \\\n",
    "            tf.nn.sigmoid(tf.add(tf.matmul(batch_normed, weights['out_mean']), \n",
    "                                 biases['out_mean']))\n",
    "        return x_reconstr_mean\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        self.reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(1e-6 + self.x_reconstr_mean)\n",
    "                           + (1-self.x) * tf.log(1e-6 + 1 - self.x_reconstr_mean),\n",
    "                           1)\n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        self.latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(self.reconstr_loss + self.latent_loss)   # average over batch\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        mu = self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "        sigma = self.sess.run(self.z_log_sigma_sq, feed_dict={self.x: X}) ###!!! (doğru mu/gerekli mi??)\n",
    "        return mu, sigma\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})\n",
    "    \n",
    "    \n",
    "    def reconstruct_error(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run((self.reconstr_loss, self.latent_loss), \n",
    "                             feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, implementing a VAE in tensorflow is relatively straightforward (in particular since we don not need to code the gradient computation). A bit confusing is potentially that all the logic happens at initialization of the class (where the graph is generated), while the actual sklearn interface methods are very simple one-liners.\n",
    "\n",
    "We can now define a simple fuction which trains the VAE using mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(network_architecture, \n",
    "          learning_rate=0.0001,\n",
    "          batch_size=1024, \n",
    "          training_epochs=70, \n",
    "          display_step=1):\n",
    "    \n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    history_loss = dict()\n",
    "    \n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        \n",
    "        loss_batch = []\n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, _ = kdd99.train_next_batch(batch_size) #mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = vae.partial_fit(batch_xs)\n",
    "            \n",
    "            loss_batch.append(cost)\n",
    "            \n",
    "            #print(\" batch:\",i,\"cost:\",cost)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"--- end of epoch:\", '%04d' % (epoch+1), \n",
    "                  \"avg cost=\", \"{:.9f}\".format(avg_cost),\n",
    "                  \"time=\", \"{:.3f} seconds\".format(end-start), )\n",
    "            \n",
    "        ### append loss to history\n",
    "        history_loss[str(epoch)] = loss_batch\n",
    "        history_loss[str(epoch)+\"-avg\"] = avg_cost\n",
    "            \n",
    "    return vae, history_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrating reconstruction quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a VAE on MNIST by just specifying the network topology. We start with training a VAE with a 20-dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- end of epoch: 0001 avg cost= 70.265295904 time= 4.162 seconds\n",
      "--- end of epoch: 0002 avg cost= 27.693592305 time= 3.488 seconds\n",
      "--- end of epoch: 0003 avg cost= 12.843173820 time= 3.593 seconds\n",
      "--- end of epoch: 0004 avg cost= 11.207451733 time= 3.536 seconds\n",
      "--- end of epoch: 0005 avg cost= 10.790388352 time= 3.322 seconds\n",
      "--- end of epoch: 0006 avg cost= 10.424556433 time= 3.352 seconds\n",
      "--- end of epoch: 0007 avg cost= 9.282161063 time= 3.417 seconds\n",
      "--- end of epoch: 0008 avg cost= 6.719495321 time= 3.504 seconds\n",
      "--- end of epoch: 0009 avg cost= 5.126751181 time= 3.353 seconds\n",
      "--- end of epoch: 0010 avg cost= 4.696079080 time= 3.352 seconds\n",
      "--- end of epoch: 0011 avg cost= 4.553808110 time= 3.478 seconds\n",
      "--- end of epoch: 0012 avg cost= 4.496733281 time= 3.674 seconds\n",
      "--- end of epoch: 0013 avg cost= 4.415130458 time= 3.473 seconds\n",
      "--- end of epoch: 0014 avg cost= 4.395545683 time= 3.527 seconds\n",
      "--- end of epoch: 0015 avg cost= 4.291409600 time= 3.598 seconds\n",
      "--- end of epoch: 0016 avg cost= 4.226805545 time= 3.636 seconds\n",
      "--- end of epoch: 0017 avg cost= 4.126725355 time= 3.394 seconds\n",
      "--- end of epoch: 0018 avg cost= 4.004142371 time= 3.447 seconds\n",
      "--- end of epoch: 0019 avg cost= 3.873738449 time= 3.382 seconds\n",
      "--- end of epoch: 0020 avg cost= 3.717436972 time= 3.389 seconds\n",
      "--- end of epoch: 0021 avg cost= 3.610016048 time= 3.400 seconds\n",
      "--- end of epoch: 0022 avg cost= 3.523747532 time= 3.468 seconds\n",
      "--- end of epoch: 0023 avg cost= 3.386387463 time= 3.535 seconds\n",
      "--- end of epoch: 0024 avg cost= 3.329487378 time= 3.517 seconds\n",
      "--- end of epoch: 0025 avg cost= 3.259563697 time= 3.422 seconds\n",
      "--- end of epoch: 0026 avg cost= 3.182654353 time= 3.462 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=60, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=40, # 2nd layer encoder neurons\n",
    "         n_hidden_recog_3=20, # 2nd layer encoder neurons\n",
    "         n_hidden_recog_4=10, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=10, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=20, # 2nd layer decoder neurons\n",
    "         n_hidden_gener_3=40, # 1st layer decoder neurons\n",
    "         n_hidden_gener_4=60, # 2nd layer decoder neurons\n",
    "         n_input=120,  # kdd99 data input dimension\n",
    "         n_z=5,\n",
    "        )  # dimensionality of latent space\n",
    "\n",
    "vae, history_loss = train(network_architecture, \n",
    "            training_epochs=100,\n",
    "            learning_rate = 0.0001,\n",
    "            display_step=1, \n",
    "            batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save network params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights(filename = \"./\" + timestamp + \"-tfsave.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = timestamp + \"_history\"\n",
    "np.savez_compressed(filename, history_loss=history_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this we can sample some test inputs and visualize how well the VAE can reconstruct those. In general the VAE does really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample =  kdd99.test_next_batch(1024)[0]  #mnist.test.next_batch(100)[0]\n",
    "x_reconstruct = vae.reconstruct(x_sample)\n",
    "\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(10, 12), vmin=0, vmax=1, cmap=cmap)\n",
    "    plt.title(\"Test input\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(10, 12), vmin=0, vmax=1, cmap=cmap)\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-reconstruction.png\".format(hist_bins)\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"../datasets/kddcup/kdd99_test-randomState_None.npz\") as data:  ### kdd99_test\n",
    "    x_test = data['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = x_test[np.where(x_test[:,-1] == 1)]\n",
    "\n",
    "anomaly_data = x_test[np.where(x_test[:,-1] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anomaly_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain sample energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Energies from normal class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_normal = []\n",
    "\n",
    "### from first batch until last batch (not processing last batch)\n",
    "for i in range(0, len(normal_data)//batch_size -1):\n",
    "    start_idx = i * batch_size\n",
    "      \n",
    "    batch = normal_data[start_idx : start_idx+batch_size]\n",
    "\n",
    "    print(\"running step:\",i)\n",
    "        \n",
    "    losses_normal.append(vae.reconstruct_error(batch))\n",
    "    \n",
    "    \n",
    "### TODO: process last batch\n",
    "\n",
    "losses_normal = np.asarray(losses_normal)\n",
    "\n",
    "loss_reconstr_normal =  losses_normal[:,0,:]\n",
    "loss_latent_normal =  losses_normal[:,1,:]\n",
    "\n",
    "loss_reconstr_normal = loss_reconstr_normal.ravel()\n",
    "loss_latent_normal = loss_latent_normal.ravel() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print the range in losses\n",
    "print(loss_latent_normal.min())\n",
    "print(loss_latent_normal.max())\n",
    "\n",
    "print(loss_reconstr_normal.min())\n",
    "print(loss_reconstr_normal.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bins = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### plot losses\n",
    "\n",
    "plt.hist(loss_latent_normal, bins=hist_bins, log=True)\n",
    "\n",
    "plt.title(\"Latent losses for \\\"normal\\\" class\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-latent-normalHist_bins{}.png\".format(hist_bins)\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loss_reconstr_normal, bins=hist_bins, log=True)\n",
    "\n",
    "plt.title(\"Reconstruction losses for \\\"normal\\\" class\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-reconst-normalHist_bins{}.png\".format(hist_bins)\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Energies from anomaly class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_anomaly = []\n",
    "\n",
    "### from first batch until last batch (not processing last batch)\n",
    "for i in range(0, len(anomaly_data)//batch_size - 1):\n",
    "    start_idx = i * batch_size\n",
    "      \n",
    "    batch = anomaly_data[start_idx : start_idx+batch_size]\n",
    "\n",
    "    print(\"running step:\",i)\n",
    "    \n",
    "    losses_anomaly.append(vae.reconstruct_error(batch))\n",
    "    \n",
    "    \n",
    "### TODO: process last batch\n",
    "\n",
    "\n",
    "losses_anomaly = np.asarray(losses_anomaly)\n",
    "\n",
    "loss_reconstr_anomaly =  losses_anomaly[:,0,:]\n",
    "loss_latent_anomaly =  losses_anomaly[:,1,:]\n",
    "\n",
    "loss_reconstr_anomaly = loss_reconstr_anomaly.ravel()\n",
    "loss_latent_anomaly = loss_latent_anomaly.ravel() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print the range in losses\n",
    "print(loss_latent_anomaly.min())\n",
    "print(loss_latent_anomaly.max())\n",
    "\n",
    "print(loss_reconstr_anomaly.min())\n",
    "print(loss_reconstr_anomaly.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### plot losses\n",
    "\n",
    "plt.hist(loss_latent_anomaly, bins=hist_bins, color=\"red\", log=True)\n",
    "\n",
    "plt.title(\"Latent losses for \\\"anomaly\\\" class\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-latent-anomalyHist_bins{}.png\".format(hist_bins)\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loss_reconstr_anomaly, bins=hist_bins, color=\"red\", log=True)\n",
    "\n",
    "plt.title(\"Reconstruction losses for \\\"anomaly\\\" class\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-reconst-anomalyHist_bins{}.png\".format(hist_bins)\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show both normal and anomaly in same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_bins = 120\n",
    "\n",
    "plt.hist((loss_reconstr_normal, loss_reconstr_anomaly), \n",
    "         bins=hist_bins, \n",
    "         color=[\"blue\", \"red\"], \n",
    "         histtype=\"bar\", \n",
    "         label=[\"normal\", \"anomaly\"], \n",
    "         rwidth=1.0,\n",
    "         stacked=False,\n",
    "         log=True)\n",
    "\n",
    "plt.title(\"Reconstruction losses\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-reconstr-allHist_bins{}.png\".format(hist_bins)\n",
    "plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_bins = 120\n",
    "\n",
    "plt.hist((loss_latent_normal, loss_latent_anomaly), \n",
    "         bins=hist_bins, \n",
    "         color=[\"blue\", \"red\"], \n",
    "         histtype=\"bar\", \n",
    "         label=[\"normal\", \"anomaly\"], \n",
    "         rwidth=1.0,\n",
    "         stacked=False,\n",
    "         log=True)\n",
    "\n",
    "plt.title(\"Latent losses\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = timestamp + \"-latent-allHist_bins{}.png\".format(hist_bins)\n",
    "plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the threshold for classifying as anomaly: \"top %20 of highest energy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_all = np.concatenate((energies_normal, energies_anomaly), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort calculated energies descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_energies = np.sort(energies_all)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top %20 percent is the first %20 part of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_index = int(np.floor(len(sorted_energies) * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_energies[:threshold_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = sorted_energies[threshold_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below this threshold, samples are classified as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Anomaly class is positive\" (from paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                   | anomaly | normal |   |   |\n",
    "|-------------------|---------|--------|---|---|\n",
    "| predicted anomaly | TP      | FP     |   |   |\n",
    "| predicted normal  | FN      | TN     |   |   |\n",
    "|                   |         |        |   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = len(energies_anomaly[energies_anomaly > threshold])\n",
    "fp = len(energies_normal[energies_normal > threshold])\n",
    "\n",
    "tn = len(energies_normal[energies_normal < threshold])\n",
    "fn = len(energies_anomaly[energies_anomaly < threshold])\n",
    "\n",
    "print('tp', tp)\n",
    "print('fp', fp)\n",
    "print('tn', tn)\n",
    "print('fn', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2*tp / (2*tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrating latent space (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a VAE with 2d latent space and illustrates how the encoder (the recognition network) encodes some of the labeled inputs (collapsing the Gaussian distribution in latent space to its mean). This gives us some insights into the structure of the learned manifold (latent space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=60, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=40, # 2nd layer encoder neurons\n",
    "         n_hidden_recog_3=20, # 2nd layer encoder neurons\n",
    "         n_hidden_recog_4=10, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=10, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=20, # 2nd layer decoder neurons\n",
    "         n_hidden_gener_3=40, # 1st layer decoder neurons\n",
    "         n_hidden_gener_4=60, # 2nd layer decoder neurons\n",
    "         n_input=120,  # kdd99 data input dimension\n",
    "         n_z=2,\n",
    "        )  # dimensionality of latent space\n",
    "\n",
    "vae, history_loss = train(network_architecture, \n",
    "            training_epochs=20,\n",
    "            learning_rate = 0.0001,\n",
    "            display_step=1, \n",
    "            batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample, y_sample = kdd99.test_next_batch(5000) #mnist.test.next_batch(5000)\n",
    "z_mu,_ = vae_2d.transform(x_sample)  #### TODO:also try z_sig?\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(z_mu[:, 0], z_mu[:, 1], s=20, c=x_sample[:,-1])   ###last column has the label info\n",
    "plt.colorbar()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other way of getting insights into the latent space is to use the generator network to plot reconstrunctions at the positions in the latent space for which they have been generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = ny = 20\n",
    "x_values = np.linspace(-3, 3, nx)\n",
    "y_values = np.linspace(-3, 3, ny)\n",
    "\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "canvas = np.empty((10*ny, 12*nx))\n",
    "for i, yi in enumerate(x_values):\n",
    "    for j, xi in enumerate(y_values):\n",
    "        z_mu = np.array([[xi, yi]]*vae_2d.batch_size)\n",
    "        x_mean = vae_2d.generate(z_mu)\n",
    "        canvas[(nx-i-1)*10:(nx-i)*10, j*12:(j+1)*12] = x_mean[0].reshape(10, 12)\n",
    "\n",
    "plt.figure(figsize=(12, 14))        \n",
    "Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=cmap)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
