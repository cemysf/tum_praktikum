{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to end train both networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotirmaysenapati/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, Lambda, Dropout, Concatenate, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "experiment_id = 0\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "epoch_list = [200, 20000, 10000, 4000]\n",
    "batch_size_list = [1024, 1024, 128, 1024]\n",
    "\n",
    "lambda_1 = 0.1       \n",
    "lambda_2 = 0.005     \n",
    "\n",
    "#################\n",
    "\n",
    "epochs = 100 ###epoch_list[experiment_id]\n",
    "batch_size = batch_size_list[experiment_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define compresion network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(batch_shape=(batch_size,120), name='input_placeholder')\n",
    "\n",
    "encoded = Dense(60, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(input_data)\n",
    "encoded = Dense(30, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(encoded)\n",
    "encoded = Dense(10, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(encoded)\n",
    "\n",
    "layer_lowdim = Dense(1, activation='linear', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001), name='lowdim')(encoded)\n",
    "\n",
    "decoded = Dense(10, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(layer_lowdim)\n",
    "decoded = Dense(30, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(decoded)\n",
    "decoded = Dense(60, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(decoded)\n",
    "decoded = Dense(120, activation='linear', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001), name='reconstructed')(decoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a_b):\n",
    "    '''\n",
    "    a: batch x 120\n",
    "    b: batch x 120 \n",
    "    \n",
    "    output: batch x 1\n",
    "    '''\n",
    "    a, b = a_b\n",
    "    \n",
    "    norm_a = K.sqrt(K.sum(a ** 2, axis=-1))\n",
    "    norm_b = K.sqrt(K.sum(b ** 2, axis=-1))\n",
    "    \n",
    "    out = K.sum(a * b, axis=-1) / (norm_a * norm_b)\n",
    "    out = K.reshape(out, [batch_size, 1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def relative_euc_dist(a_b):\n",
    "    '''\n",
    "    a: batch x 120\n",
    "    b: batch x 120 \n",
    "    \n",
    "    output: batch x 1\n",
    "    '''\n",
    "    a,b = a_b\n",
    "    \n",
    "    norm_diff = K.sqrt(K.sum((a - b)**2, axis=-1))\n",
    "    norm_a = K.sqrt(K.sum(a ** 2, axis=-1))\n",
    "    \n",
    "    out = norm_diff / norm_a\n",
    "    out = K.reshape(out, [batch_size, 1])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_cossim = Lambda(cos_sim,\n",
    "                      name='cos_sim')([input_data, decoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relativeEuc = Lambda(relative_euc_dist, \n",
    "                           name='relative_euc_dist')([input_data, decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Obtain \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funct_concat(tensors):\n",
    "    return K.concatenate(tensors)\n",
    "\n",
    "layer_concat = Lambda(funct_concat, name=\"z\")([layer_lowdim, layer_cossim, layer_relativeEuc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define estimation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_est = layer_concat #Input(shape=(3,))(layer_concat)\n",
    "\n",
    "est_layer = Dense(10, activation='tanh', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001))(input_est)   ####(input_est)\n",
    "est_layer = Dropout(0.5)(est_layer)\n",
    "est_output = Dense(4, activation='softmax', kernel_regularizer=l2(0.00001), bias_regularizer=l2(0.00001), name='gamma')(est_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyotirmaysenapati/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ga..., inputs=Tensor(\"in...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "full_network = Model(input=input_data, outputs=est_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_placeholder (InputLayer)  (1024, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (1024, 60)           7260        input_placeholder[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (1024, 30)           1830        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (1024, 10)           310         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lowdim (Dense)                  (1024, 1)            11          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (1024, 10)           20          lowdim[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1024, 30)           330         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (1024, 60)           1860        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reconstructed (Dense)           (1024, 120)          7320        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cos_sim (Lambda)                (1024, 1)            0           input_placeholder[0][0]          \n",
      "                                                                 reconstructed[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "relative_euc_dist (Lambda)      (1024, 1)            0           input_placeholder[0][0]          \n",
      "                                                                 reconstructed[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (1024, 3)            0           lowdim[0][0]                     \n",
      "                                                                 cos_sim[0][0]                    \n",
      "                                                                 relative_euc_dist[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (1024, 10)           40          z[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (1024, 10)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gamma (Dense)                   (1024, 4)            44          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,025\n",
      "Trainable params: 19,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load already saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('../../datasets/kddcup/kdd99_train-randomState_None.npz') as data: \n",
    "    x_train = data[\"x_train\"]\n",
    "    y_train = data[\"y_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198365, 120)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198365, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k: number of clusters (4 for here)\n",
    "- N: batch size\n",
    "- d: Dimension of latent vector z (3 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gamma ($\\gamma$) : membership predictions (softmax output of estimation net) [$N \\times K$]\n",
    "- phi ($\\phi$): gaussian probabilities [$K$]\n",
    "- mu ($\\mu$): gaussian means [$K \\times d$]\n",
    "- sigma ($\\Sigma$): gaussian covariances [$K \\times d \\times d$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4 \n",
    "N = batch_size \n",
    "d = int(layer_concat.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GMM parameters init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = tf.get_variable(\"phi\",\n",
    "                      shape=(k),\n",
    "                      dtype=tf.float32,\n",
    "                      initializer=tf.zeros_initializer(),\n",
    "                      trainable=False)\n",
    "\n",
    "mu = tf.get_variable(\"mu\",\n",
    "                      shape=(k,d),\n",
    "                      dtype=tf.float32,\n",
    "                      initializer=tf.zeros_initializer(), \n",
    "                      trainable=False)\n",
    "\n",
    "sigma_init = np.repeat([np.eye(d, dtype=np.float32)], k, axis=0)\n",
    "\n",
    "sigma = tf.get_variable(\"sigma\",\n",
    "                      shape=(k,d,d),\n",
    "                      dtype=tf.float32,\n",
    "                      initializer=tf.constant_initializer(sigma_init),\n",
    "                      trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_sess.run([phi.initializer,\n",
    "             mu.initializer, \n",
    "             sigma.initializer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for uninit vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf_sess.run(tf.report_uninitialized_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printGmmParams():\n",
    "    print(\"#### GMM params ####\")\n",
    "    print(\"phi:\\n\",K.eval(phi),\"\\n\")\n",
    "    print(\"mu:\\n\",K.eval(mu),\"\\n\")\n",
    "    print(\"sigma:\\n\", K.eval(sigma),\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveGmmParams(filename = \"gmmParams.npz\"):\n",
    "    phi_ = K.eval(phi)\n",
    "    mu_ = K.eval(mu)\n",
    "    sigma_ = K.eval(sigma)\n",
    "    np.savez_compressed(filename, phi=phi_, mu=mu_, sigma=sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### GMM params ####\n",
      "phi:\n",
      " [0. 0. 0. 0.] \n",
      "\n",
      "mu:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]] \n",
      "\n",
      "sigma:\n",
      " [[[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "printGmmParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "isVerbose_en = True\n",
    "isVerbose_loss = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the first time run of the totalLoss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = np.random.choice(len(x_train), size=batch_size, replace=False)\n",
    "\n",
    "batch_x_train = x_train[choices]\n",
    "batch_y_train = y_train[choices]\n",
    "\n",
    "batch_train = (batch_x_train, batch_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define energy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEnergy(z_i):\n",
    "    \"\"\"\n",
    "    compute E(z_i) in loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    #inside_sum = 0\n",
    "    inside_sum = tf.zeros(()) \n",
    "    for cluster in range(k):\n",
    "        diff = tf.reshape(z_i - mu[cluster], (1,-1))   ### (1,3)\n",
    "        diff_t = tf.reshape(diff, (-1,1)) #diff.reshape(-1,1)   ### (3,1)\n",
    "\n",
    "        sigma_inv = tf.linalg.inv(sigma[cluster]) ### (3,3)\n",
    "        \n",
    "        exp_term = tf.exp(-0.5 * tf.matmul(diff, tf.matmul(sigma_inv, diff_t)))    ### (1,1)\n",
    "\n",
    "        denom = tf.sqrt(tf.linalg.det(2 * np.pi * sigma[cluster]))\n",
    "        \n",
    "        inside_sum += phi[cluster] * (exp_term / denom)   ### (1,1)\n",
    "        \n",
    "        \n",
    "    inside_sum = tf.reshape(inside_sum, ())\n",
    "    sample_energy = -tf.log(inside_sum + 1e-6, name=\"sample_energy\")\n",
    "    \n",
    "        \n",
    "    ### flatten inside_sum and return log of it\n",
    "    return sample_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalLoss(yTrue, yPred):\n",
    "    ### autoencoder loss\n",
    "    autoenc_loss = tf.reduce_sum(((input_data - decoded)**2), axis=1)    ### (N,)\n",
    "    autoenc_loss = tf.reduce_mean(autoenc_loss, axis=0)      #### mean over all N in batch\n",
    "    \n",
    "    ### obtain z and gamma for current batch\n",
    "    z = layer_concat\n",
    "    gamma = est_output\n",
    "    \n",
    "    \n",
    "    ########### gmm update #################\n",
    "    gamma = est_output ### + 1e-6\n",
    "    z = layer_concat         \n",
    "\n",
    "    ### update list\n",
    "    updates_gmm = []\n",
    "\n",
    "    ################### phi #################\n",
    "    update_phi = tf.assign(phi, \n",
    "                           tf.reduce_sum(gamma/batch_size, axis=0), \n",
    "                           name=\"update_phi\")\n",
    "    updates_gmm.append(update_phi)\n",
    "    #######################################\n",
    "\n",
    "    ################## mu ################\n",
    "    for cluster in range(k):\n",
    "        ### get the corresponding column of predictions\n",
    "        gamma_cluster = tf.reshape(gamma[:,cluster], (-1,1))    ### (N x 1)\n",
    "\n",
    "        ### duplicate column d times\n",
    "        gamma_cluster_tile = tf.tile(gamma_cluster, (1,d))   ### (N x d)\n",
    "\n",
    "        ### sum over all batch and divide\n",
    "        matmul = tf.matmul(z, gamma_cluster_tile, transpose_a=True)   ### (dxd)\n",
    "\n",
    "        result = tf.reduce_sum(matmul, axis=0, name=\"mu_matmul_red\") / tf.reduce_sum(gamma[:,cluster], axis = 0, name=\"mu_gamma_red\")\n",
    "\n",
    "        update_mu = tf.assign(mu[cluster], \n",
    "                              result,\n",
    "                              name=\"update_mu\"+str(cluster))\n",
    "        updates_gmm.append(update_mu)\n",
    "    #########################################\n",
    "\n",
    "    ################ sigma ###############\n",
    "    for cluster in range(k):\n",
    "        ### expand gamma for each sample\n",
    "        gamma_cluster = tf.reshape(gamma[:,cluster], (-1,1))    ### (N x 1)\n",
    "        gamma_cluster_expand = tf.expand_dims(gamma_cluster, 1) ### (N x 1 x 1)\n",
    "\n",
    "\n",
    "        #### calculating diff\n",
    "        ## expand mu and z\n",
    "        ######## TODO: race condition??? (get mu after its updated value (auto??))\n",
    "        with tf.control_dependencies(updates_gmm):\n",
    "            mu_cluster = tf.reshape(mu[cluster], (1,-1))   ### (1 x d)\n",
    "\n",
    "        mu_expand = tf.expand_dims(mu_cluster, 1)      ### (1 x 1 x 3)\n",
    "        mu_expand_tile = tf.tile(mu_expand, tf.stack([N, 1, 1]))   ### (N x 1 x d)\n",
    "\n",
    "        z_expand = tf.expand_dims(z, 1)    ### (N x 1 x d)\n",
    "\n",
    "        diff = z_expand - mu_expand_tile   ### (N x 1 x d)\n",
    "\n",
    "        ### matmul in the upper part\n",
    "        matmul = tf.matmul(diff, diff, transpose_a=True)  ### (N x d x d)\n",
    "\n",
    "\n",
    "        ### nominator\n",
    "        nom = gamma_cluster_expand * matmul   ### (N x d x d)\n",
    "        nom_reduced = tf.reduce_sum(nom, axis=0) ### (d x d)\n",
    "\n",
    "\n",
    "        ### denominator\n",
    "        denom = tf.reduce_sum(gamma_cluster)  ### single value, zero dim\n",
    "\n",
    "        update_sigma = tf.assign(sigma[cluster], \n",
    "                                 nom_reduced/denom, \n",
    "                                 name =\"update_sigma\"+str(cluster))\n",
    "        updates_gmm.append(update_sigma)\n",
    "\n",
    "    \n",
    "    \n",
    "    ### dependency control\n",
    "    with tf.control_dependencies(updates_gmm):  \n",
    "        ### sample energy   \n",
    "        sample_en_batch = tf.map_fn(lambda z_i: computeEnergy(z_i), z)\n",
    "        sample_en = tf.reduce_mean(sample_en_batch, axis=0)\n",
    "        sample_en *= lambda_1\n",
    "\n",
    "        p = tf.reduce_sum(1 / tf.matrix_diag_part(sigma))\n",
    "        p *= lambda_2\n",
    "\n",
    "    ### total loss\n",
    "    total_loss = autoenc_loss + sample_en + p\n",
    "\n",
    "    return total_loss  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load previos weights\n",
    "### full_network.load_weights(\"modelsave_weights-epochs5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### keras\n",
    "\n",
    "adam = optimizers.adam(lr=learning_rate, clipnorm=1., clipvalue=0.5) \n",
    "\n",
    "full_network.compile(optimizer=adam, loss=totalLoss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-29_19:10 \n"
     ]
    }
   ],
   "source": [
    "### Timestamp\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "print(timestamp, \"\")\n",
    "\n",
    "directory = '../../models/kddcup/'+timestamp\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch generator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: optimization??\n",
    "\n",
    "def batchGenerator():\n",
    "    '''\n",
    "    return: number of batch_size examples in each run\n",
    "    '''\n",
    "    \n",
    "    while True:\n",
    "        choices = np.random.choice(len(x_train), size=batch_size, replace=False)\n",
    "        \n",
    "        batch_x_train = x_train[choices]\n",
    "        batch_y_train = y_train[choices]\n",
    "        \n",
    "        \n",
    "        yield (batch_x_train, batch_y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training using fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(x_train.shape[0] / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "194/194 [==============================] - 209s 1s/step - loss: 14.4402\n",
      "Epoch 2/100\n",
      "194/194 [==============================] - 197s 1s/step - loss: 2.1388\n",
      "Epoch 3/100\n",
      "194/194 [==============================] - 218s 1s/step - loss: 1.2632\n",
      "Epoch 4/100\n",
      "194/194 [==============================] - 208s 1s/step - loss: 0.9938\n",
      "Epoch 5/100\n",
      "194/194 [==============================] - 170s 877ms/step - loss: 0.9314\n",
      "Epoch 6/100\n",
      "194/194 [==============================] - 193s 995ms/step - loss: 1.1053\n",
      "Epoch 7/100\n",
      "194/194 [==============================] - 190s 980ms/step - loss: 0.9184\n",
      "Epoch 8/100\n",
      "194/194 [==============================] - 204s 1s/step - loss: 0.7683\n",
      "Epoch 9/100\n",
      "194/194 [==============================] - 181s 935ms/step - loss: 0.7322\n",
      "Epoch 10/100\n",
      "194/194 [==============================] - 184s 946ms/step - loss: 0.7484\n",
      "Epoch 11/100\n",
      "194/194 [==============================] - 155s 797ms/step - loss: 0.7968\n",
      "Epoch 12/100\n",
      "194/194 [==============================] - 151s 779ms/step - loss: 0.8364\n",
      "Epoch 13/100\n",
      "194/194 [==============================] - 145s 747ms/step - loss: 0.8476\n",
      "Epoch 14/100\n",
      "194/194 [==============================] - 135s 697ms/step - loss: 0.8962\n",
      "Epoch 15/100\n",
      "194/194 [==============================] - 163s 842ms/step - loss: 0.9417\n",
      "Epoch 16/100\n",
      "194/194 [==============================] - 144s 742ms/step - loss: 1.1012\n",
      "Epoch 17/100\n",
      "194/194 [==============================] - 147s 759ms/step - loss: 1.5273\n",
      "Epoch 18/100\n",
      "194/194 [==============================] - 148s 762ms/step - loss: 2.0757\n",
      "Epoch 19/100\n",
      "194/194 [==============================] - 174s 897ms/step - loss: 2.3729\n",
      "Epoch 20/100\n",
      "194/194 [==============================] - 150s 771ms/step - loss: 2.5670\n",
      "Epoch 21/100\n",
      "194/194 [==============================] - 134s 692ms/step - loss: 2.5081\n",
      "Epoch 22/100\n",
      "194/194 [==============================] - 134s 689ms/step - loss: 2.3638\n",
      "Epoch 23/100\n",
      "194/194 [==============================] - 141s 729ms/step - loss: 2.1526\n",
      "Epoch 24/100\n",
      "194/194 [==============================] - 144s 744ms/step - loss: 1.8863\n",
      "Epoch 25/100\n",
      "194/194 [==============================] - 134s 693ms/step - loss: 1.6590\n",
      "Epoch 26/100\n",
      "194/194 [==============================] - 4882s 25s/step - loss: 1.4926\n",
      "Epoch 27/100\n",
      "194/194 [==============================] - 155s 800ms/step - loss: 1.3775\n",
      "Epoch 28/100\n",
      "194/194 [==============================] - 145s 748ms/step - loss: 1.2526\n",
      "Epoch 29/100\n",
      "194/194 [==============================] - 154s 794ms/step - loss: 1.1496\n",
      "Epoch 30/100\n",
      "194/194 [==============================] - 140s 723ms/step - loss: 1.0508\n",
      "Epoch 31/100\n",
      "194/194 [==============================] - 133s 685ms/step - loss: 0.9751\n",
      "Epoch 32/100\n",
      "194/194 [==============================] - 149s 767ms/step - loss: 0.9081\n",
      "Epoch 33/100\n",
      "194/194 [==============================] - 172s 886ms/step - loss: 0.8373\n",
      "Epoch 34/100\n",
      "194/194 [==============================] - 168s 865ms/step - loss: 0.7897\n",
      "Epoch 35/100\n",
      "194/194 [==============================] - 172s 888ms/step - loss: 0.7552\n",
      "Epoch 36/100\n",
      "194/194 [==============================] - 171s 879ms/step - loss: 0.7167\n",
      "Epoch 37/100\n",
      "194/194 [==============================] - 171s 880ms/step - loss: 0.6995\n",
      "Epoch 38/100\n",
      "194/194 [==============================] - 171s 881ms/step - loss: 0.6773\n",
      "Epoch 39/100\n",
      "194/194 [==============================] - 172s 884ms/step - loss: 0.6600\n",
      "Epoch 40/100\n",
      "194/194 [==============================] - 133s 685ms/step - loss: 0.6371\n",
      "Epoch 41/100\n",
      "194/194 [==============================] - 132s 678ms/step - loss: 0.6119\n",
      "Epoch 42/100\n",
      "194/194 [==============================] - 160s 823ms/step - loss: 0.5926\n",
      "Epoch 43/100\n",
      "194/194 [==============================] - 178s 916ms/step - loss: 0.5827\n",
      "Epoch 44/100\n",
      "194/194 [==============================] - 176s 908ms/step - loss: 0.5790\n",
      "Epoch 45/100\n",
      "194/194 [==============================] - 179s 921ms/step - loss: 0.5660\n",
      "Epoch 46/100\n",
      "194/194 [==============================] - 156s 805ms/step - loss: 0.5507\n",
      "Epoch 47/100\n",
      "194/194 [==============================] - 144s 742ms/step - loss: 0.5494\n",
      "Epoch 48/100\n",
      "194/194 [==============================] - 129s 664ms/step - loss: 0.5321\n",
      "Epoch 49/100\n",
      "194/194 [==============================] - 119s 615ms/step - loss: 0.5364\n",
      "Epoch 50/100\n",
      "194/194 [==============================] - 117s 605ms/step - loss: 0.5268\n",
      "Epoch 51/100\n",
      "194/194 [==============================] - 118s 606ms/step - loss: 0.5208\n",
      "Epoch 52/100\n",
      "194/194 [==============================] - 135s 695ms/step - loss: 0.5174\n",
      "Epoch 53/100\n",
      "194/194 [==============================] - 154s 791ms/step - loss: 0.5077\n",
      "Epoch 54/100\n",
      "194/194 [==============================] - 173s 893ms/step - loss: 0.5110\n",
      "Epoch 55/100\n",
      "194/194 [==============================] - 154s 794ms/step - loss: 0.4990\n",
      "Epoch 56/100\n",
      "194/194 [==============================] - 154s 796ms/step - loss: 0.4972\n",
      "Epoch 57/100\n",
      "194/194 [==============================] - 154s 793ms/step - loss: 0.5063\n",
      "Epoch 58/100\n",
      "194/194 [==============================] - 154s 796ms/step - loss: 0.5088\n",
      "Epoch 59/100\n",
      "194/194 [==============================] - 155s 800ms/step - loss: 0.5223\n",
      "Epoch 60/100\n",
      "194/194 [==============================] - 154s 795ms/step - loss: 0.5252\n",
      "Epoch 61/100\n",
      "194/194 [==============================] - 156s 804ms/step - loss: 0.5359\n",
      "Epoch 62/100\n",
      "194/194 [==============================] - 155s 801ms/step - loss: 0.5371\n",
      "Epoch 63/100\n",
      "194/194 [==============================] - 156s 805ms/step - loss: 0.5472\n",
      "Epoch 64/100\n",
      "194/194 [==============================] - 154s 795ms/step - loss: 0.5557\n",
      "Epoch 65/100\n",
      "194/194 [==============================] - 154s 794ms/step - loss: 0.5505\n",
      "Epoch 66/100\n",
      "194/194 [==============================] - 154s 796ms/step - loss: 0.5600\n",
      "Epoch 67/100\n",
      "194/194 [==============================] - 155s 797ms/step - loss: 0.5574\n",
      "Epoch 68/100\n",
      "194/194 [==============================] - 126s 650ms/step - loss: 0.5698\n",
      "Epoch 69/100\n",
      "194/194 [==============================] - 117s 602ms/step - loss: 0.5786\n",
      "Epoch 70/100\n",
      "194/194 [==============================] - 117s 603ms/step - loss: 0.5759\n",
      "Epoch 71/100\n",
      "194/194 [==============================] - 150s 775ms/step - loss: 0.5850\n",
      "Epoch 72/100\n",
      "194/194 [==============================] - 151s 776ms/step - loss: 0.5870\n",
      "Epoch 73/100\n",
      "194/194 [==============================] - 151s 776ms/step - loss: 0.5966\n",
      "Epoch 74/100\n",
      "194/194 [==============================] - 151s 777ms/step - loss: 0.6144\n",
      "Epoch 75/100\n",
      "194/194 [==============================] - 151s 780ms/step - loss: 0.6155\n",
      "Epoch 76/100\n",
      "194/194 [==============================] - 167s 862ms/step - loss: 0.6293\n",
      "Epoch 77/100\n",
      "194/194 [==============================] - 128s 658ms/step - loss: 0.6343\n",
      "Epoch 78/100\n",
      "194/194 [==============================] - 130s 669ms/step - loss: 0.6436\n",
      "Epoch 79/100\n",
      "194/194 [==============================] - 122s 627ms/step - loss: 0.6542\n",
      "Epoch 80/100\n",
      "194/194 [==============================] - 120s 619ms/step - loss: 0.6703\n",
      "Epoch 81/100\n",
      "194/194 [==============================] - 122s 629ms/step - loss: 0.6802\n",
      "Epoch 82/100\n",
      "194/194 [==============================] - 124s 638ms/step - loss: 0.6880\n",
      "Epoch 83/100\n",
      "194/194 [==============================] - 127s 657ms/step - loss: 0.6872\n",
      "Epoch 84/100\n",
      "194/194 [==============================] - 120s 620ms/step - loss: 0.6913\n",
      "Epoch 85/100\n",
      "194/194 [==============================] - 123s 632ms/step - loss: 0.6884\n",
      "Epoch 86/100\n",
      "194/194 [==============================] - 122s 627ms/step - loss: 0.6844\n",
      "Epoch 87/100\n",
      "194/194 [==============================] - 126s 651ms/step - loss: 0.6836\n",
      "Epoch 88/100\n",
      "194/194 [==============================] - 121s 624ms/step - loss: 0.6870\n",
      "Epoch 89/100\n",
      "194/194 [==============================] - 128s 659ms/step - loss: 0.6895\n",
      "Epoch 90/100\n",
      "194/194 [==============================] - 127s 654ms/step - loss: 0.7002\n",
      "Epoch 91/100\n",
      "194/194 [==============================] - 118s 610ms/step - loss: 0.7075\n",
      "Epoch 92/100\n",
      "194/194 [==============================] - 118s 608ms/step - loss: 0.7102\n",
      "Epoch 93/100\n",
      "194/194 [==============================] - 132s 680ms/step - loss: 0.7158\n",
      "Epoch 94/100\n",
      "194/194 [==============================] - 154s 796ms/step - loss: 0.7240\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 132s 680ms/step - loss: 0.7373\n",
      "Epoch 96/100\n",
      "194/194 [==============================] - 124s 638ms/step - loss: 0.7482\n",
      "Epoch 97/100\n",
      "194/194 [==============================] - 121s 626ms/step - loss: 0.7674\n",
      "Epoch 98/100\n",
      "194/194 [==============================] - 119s 613ms/step - loss: 0.7892\n",
      "Epoch 99/100\n",
      "194/194 [==============================] - 120s 620ms/step - loss: 0.8131\n",
      "Epoch 100/100\n",
      "194/194 [==============================] - 154s 796ms/step - loss: 0.8294\n"
     ]
    }
   ],
   "source": [
    "history = full_network.fit_generator(batchGenerator(),\n",
    "                                     epochs = epochs,\n",
    "                                     steps_per_epoch = steps_per_epoch,\n",
    "                                     verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_network.save_weights('../../models/kddcup/{}/'.format(timestamp)+timestamp+\"-modelsave_weights-epochs{}.h5\".format(epochs), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_network.save('../../models/kddcup/{}/'.format(timestamp)+timestamp+\"-modelsave-epochs{}.h5\".format(epochs), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_np = np.asarray(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../models/kddcup/{}/'.format(timestamp)+timestamp + \"-history.npz\", history = history_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Last gmm params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### GMM params ####\n",
      "phi:\n",
      " [0.25 0.25 0.25 0.25] \n",
      "\n",
      "mu:\n",
      " [[1.1386565 1.1386565 1.1386565]\n",
      " [1.1386565 1.1386565 1.1386565]\n",
      " [1.1386565 1.1386565 1.1386565]\n",
      " [1.1386565 1.1386565 1.1386565]] \n",
      "\n",
      "sigma:\n",
      " [[[1.2112758  0.1714626  1.1308527 ]\n",
      "  [0.1714626  0.02834561 0.15191464]\n",
      "  [1.1308527  0.15191464 1.0722297 ]]\n",
      "\n",
      " [[1.2112758  0.1714626  1.1308527 ]\n",
      "  [0.1714626  0.02834561 0.15191464]\n",
      "  [1.1308527  0.15191464 1.0722297 ]]\n",
      "\n",
      " [[1.2112758  0.1714626  1.1308527 ]\n",
      "  [0.1714626  0.02834561 0.15191464]\n",
      "  [1.1308527  0.15191464 1.0722297 ]]\n",
      "\n",
      " [[1.2112758  0.1714626  1.1308527 ]\n",
      "  [0.1714626  0.02834561 0.15191464]\n",
      "  [1.1308527  0.15191464 1.0722297 ]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "printGmmParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gmm save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveGmmParams(filename= '../../models/kddcup/{}/'.format(timestamp)+timestamp + \"-gmmParams.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-29_19:10\n"
     ]
    }
   ],
   "source": [
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
